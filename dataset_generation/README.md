# Dataset Generation
In this section, we aim to generate the dataset for the next phase of the project. In order to run the scripts properly, you need to install Defects4J and its dependencies. You can follow the steps from the official Defects4J installation guide [here](https://github.com/rjust/defects4j). Please make sure that you add Defects4J's executables to your PATH.

## How to check if Defects4J has been installed correctly?
After following the steps from the official Defects4J installation guide, please execute the following in your terminal:  

`defects4j info -p Lang`

If the output is like the following, it means Defects4J is functional and ready to be used in our work. Please note that instead of `/home/ali/` you will see your own directory names.

```
Summary of configuration for Project: Lang
--------------------------------------------------------------------------------
    Script dir: /home/ali/defects4j/framework
      Base dir: /home/ali/defects4j
    Major root: /home/ali/defects4j/major
      Repo dir: /home/ali/defects4j/project_repos
--------------------------------------------------------------------------------
    Project ID: Lang
       Program: commons-lang
    Build file: /home/ali/defects4j/framework/projects/Lang/Lang.build.xml
--------------------------------------------------------------------------------
           Vcs: Vcs::Git
    Repository: /home/ali/defects4j/project_repos/commons-lang.git
     Commit db: /home/ali/defects4j/framework/projects/Lang/active-bugs.csv
Number of bugs: 64
--------------------------------------------------------------------------------
```

In all other cases, there is something wrong, please create an issue so we could assist you.

## How to generate the dataset?
In order to generate the whole dataset, please make sure that the following files are available inside this directory. Moreover, please change the directory by executing `cd dataset_generation`

- `gen_randoop.py`: the script which generates randoop tests.
- `javaClassMethodExtractor.py`: the script which extracts the methods of a Java class.
- `javaTestMethodExtractor.py`: the script which extracts the given method of a Java test class.
- `preprocessing.py`: the script which preprocesses the generated dataset.
- `real_faults_stat_creation.py`: the script which creates the statistics of real faults.
- `tuple_gen.py`: main script which generates the data for the next phase of the project by using above scripts.
- `triplet_gen.py`: the script which generates the triplets <T, C+, C-> from the data generated by `tuple_gen.py`.

1) You can generate randoop tests by executing the following commands. Since it takes hours to generate randoop tests for all projects, we recommend downloading the generated randoop tests from SEER's data repository on Zenodo (the link is provided in the main README). The name of the file is `defects4j_randoop_tests.zip`. Please move the downloaded .zip file under the current directory (dataset_generation) and extract it into the folder named `defects4j_randoop_tests` under this directory.
```
python3 gen_randoop.py --defects4j_path <path-to-d4j> 
```

2) After the randoop tests are generated (or you can use the randoop tests we provided), you can start generating the dataset by running the `tuple_gen.py` on both the developer written tests and the randoop tests.
```
python3 tuple_gen.py --randoop_tests False
python3 tuple_gen.py --randoop_tests True
```

3) Before preprocessing the dataset, you need to run `triplet_gen.py` in order to the give the last and processable shape of the dataset. Please note that this script takes a command line argument `type`, i.e., fcbc_randoop, fcbc_dev, fcfcm, etc.
```
python3 triplet_gen.py <type>
```  

4) The last step is to preprocess (clean, remove duplicates, etc.) the dataset.
```
python3 preprocessing.py --output_dir <output_dir> --data_path <data_path> --dataset_name <dataset_name>
```

Please run scripts with `--help` to get more information about specific usages.

## Generations are Slow?

We understand generations take a significant amount of time. In fact, we were only able to finish it in a few days. Therefore, we highly recommend everyone to use the final, cleaned, preprocessed dataset available on Zenodo.